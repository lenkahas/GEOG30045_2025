---
title: "Understanding K-Nearest Neighbors Algorithms"
format: 
  html:
    standalone: true
    embed-resources: true
---
Today, we'll look at a dataset relating the beer a person drinks to the schnitzel they're predicted to eat at Oktoberfest: 
```{r echo=F, message=F, warning=F}
library(tidyverse)
set.seed(85711)
n_samples = 24
oktoberfest <- tibble(
    beer = c(seq(0,6) + rnorm(20), seq(5,6)+rnorm(4, sd=.5)),
) |> mutate(
    schnitzel = (1.8 * beer**3 - 10*beer**2 + 2*beer + rnorm(n_samples, 150, sd=10))
) |> mutate(
    beer = round((beer - min(beer) + rgamma(1, 1))*.4, 2),
    schnitzel = round(schnitzel, 0)
)
oktoberfest |> write_csv("./oktoberfest.csv")
```

```{r, message=F}
library(tidyverse)
oktoberfest <- read_csv("./oktoberfest.csv")
```

```{r echo=F}
knitr::kable(oktoberfest |> head(5))
```
```{r}
ggplot(
        oktoberfest, 
        aes(x=beer, y=schnitzel)
    ) + 
    geom_point() + xlim(.5, 4.5)
```

:::{.callout-tip}
## Comprehension Check

1. Using the plot above, what is your guess for:
    a. how much *schnitzel* would you guess would be eaten by someone who drank 1 beer?
    b. how much *schnitzel* would you guess would be eaten by someone who drank 2.5 beers? 
    c. how much *schnitzel* would you guess would be eaten by someone who drank no beer? 
    d. how much *beer* would be drank by someone someone who ate 100 grams of schnitzel?
    e. how much *beer* would be drank by someone who ate 125 grams of schitzel? 
    c. If you had to explain how you made these guesses, what would you say you did? 
:::
# A manual KNN

Imagine that we want to predict how much schnitzel would be eaten by a person who drank *exactly* three beers. A KNN learner will construct a prediction from the *average* of the nearest *k* training observations' $y$ values. In this case, nearness is calculated *only using* $x$. This is because we might not *actually observe $y$* in the real world: remember we're focused primarly on *making predictions* in Data Science, and sometimes we might want to make predictions about things that haven't happened yet! 

So, thinking about a prediction for someone who drank exactly three beers, we can think of the following plot:

```{r, echo=F, message=F, warning=F}
oktoberfest |> mutate(
    distance = abs(3 - beer)
) |> mutate(
    nearness_rank = rank(distance)
) |> 
ggplot(
        aes(x=beer, y=schnitzel, color=distance)
    ) + 
        geom_vline(xintercept=3, color='orangered') + 
        geom_point() + 
        geom_text(aes(label=nearness_rank, color=NULL), hjust=-.4) + 
        xlim(.5, 4.5) 
```

If we had trained a $k=1$ learner, then we would pick only the nearest observation to 3 beers, and predict that the schnitzel eaten by someone who drank 3 beers is *exactly* the amount of schnitzel eaten by someone who drank the nearest to 3 beers. This is observation $1$ in the plot above, which has a value of `r oktoberfest |> mutate(d = abs(3-beer)) |> arrange(d) |> head(1) |> pull(schnitzel) |> round(2)` grams. However, if we picked the nearest $k=4$ observations: 
```{r echo=F}
oktoberfest |> mutate(d = abs(3-beer)) |> arrange(d) |> head(4) |> knitr::kable()
```

Our average would instead be `r oktoberfest |> mutate(d = abs(3-beer)) |> arrange(d) |> pull(schnitzel) |> head(4) |> mean() |> round(2)` grams. In fact, our predicted value for someone who drinks three beers really strongly depends on $k$; as you increase $k$, your prediction generally increases: 

```{r, echo=F}
oktoberfest |> mutate(
    distance = abs(3 - beer)
) |> mutate(
    nearness_cutoff = rank(distance)
) |> arrange(nearness_cutoff) |> 
    mutate(
        prediction = cumsum(schnitzel) / nearness_cutoff
    ) |>
ggplot(
    aes(x=nearness_cutoff, y=prediction)
) + 
    geom_line() + 
    geom_point()
```

This is because the points that are *nearest to 3 beers* are have relatively low schnitzel consumption. The points that are very far from 3 have much higher schnitzel consumption. 

:::{.callout-tip}
## Comprehension Check

1. What is the predicted `schnitzel` value when $k=n$? What does that value represent? 
2. Why do we see a big jump when `nearness_cutoff`$=15$?
:::

## Thinking about KNN curves

Below, we can visualize what the predicted schnizel consumption is when you pick different values of $k$. For each of these curves, we just follow a very simple rule. 

1. find a location on the $x$ axis
2. find the $k$ observations that have the most similar $x$ values
3. calculate the average $y$ value for these $k$ observations
4. this average is your prediction for $y$. 

Note that, for now, we're assuming that we're just using the standard average, so our prediction function changes as a "step" each time a new observation enters, or leaves, the $k$-nearest neighbor set. If we scan across the $x$ axis from left to right, you can see each jump as a location where this happens. More advanced applications of KNN instead use a *weighted average* of the nearest $k$ observations, such that observations that are closer to our prediction point have more weight.^[This leads you naturally into other estimators (like LOESS, kernel regression, or geographically-weighted regression), which all build on this idea. But, if you think of the "weighting" function here as being $1$ when an observation is "in" the $KNN$ set and $0$ otherwise, this is just a difference in how we pick a kernel function.]

```{r trainplot, echo=F, warning=F, message=F}
## This is **ABSOLUTELY NOT** how I want you to train models. 
## If I see you copy this code for you own assessments without
## understanding what it is doing, then I will definitely not
## mark you favorably. Sometimes, especially in R, you have to
## use *more complex* code in order to demonstrate something visually
## than is optimal for actually *doing* the analysis!!

predspace = matrix(seq(0,5,.01), ncol=1)
n_cps = length(predspace)
xt = oktoberfest |> pull(beer) |> matrix(ncol=1)
yt = oktoberfest |> pull(schnitzel) |> matrix(ncol=1)
k1 = FNN::knn.reg(
    train=xt, 
    test=predspace,
    y=yt,
    k=1
)

k4 = FNN::knn.reg(
    train=xt,
    test=predspace,
    y=yt,
    k=4
)

k12 = FNN::knn.reg(
    train=xt,
    test=predspace,
    y=yt,
    k=12
)

k18 = FNN::knn.reg(
    train=xt,
    test=predspace,
    y=yt,
    k=18
)
ggplot() + 
    geom_point(data=oktoberfest, mapping=aes(x=beer, y=schnitzel)) + 
    geom_line(aes(x=predspace, y=k1$pred), color='red') + 
    geom_line(aes(x=predspace, y=k4$pred), color='orange') + 
    geom_line(aes(x=predspace, y=k12$pred), color='blue') + 
    geom_line(aes(x=predspace, y=k18$pred), color='purple') + 
    geom_text(
        aes(
            x=5, 
            y=c(k1$pred[n_cps], k4$pred[n_cps], k12$pred[n_cps], k18$pred[n_cps]), 
            label=c("k=1","k=4","k=12","k=18"),
            vjust=-.5)
    )
```

We'll use these functions to think through the following questions: 

:::{.callout-tip}
## Comprehension Check

1. Why do the KNN learners "flatten out" at the very small and very big values of `beer`?
2. Which KNN learner do you think has the lowest error? 
3. Which KNN learner do you think represents the best tradeoff in terms of the bias-variance tradeoff? 
4. Assume that we have observed the following *validation values*:

|beer|schnitzel|
|----|---------|
|1   |127      |
|2   |127      |
|3   |98       |
|4   |225      |

Use the plot to approximately judge which model would have the best performance on this validation set. 
:::

## Training a KNN learner yourself

To optimize our KNN learner using crossvalidation, we can use the `caret` package. This is a general-purpose interface to nearly any estimator you would want to use. [This is the table of the 238 (at the time of writing) estimators](https://topepo.github.io/caret/available-models.html) supported by `caret`. To fit a KNN, you set `method=knn`. 

```{r message=F, warning=F}
library(caret)

knn = train(
    schnitzel ~ beer, 
    data=oktoberfest, 
    method='knn', 
    trControl=trainControl(method='cv')
)
```

By default, `caret` will try to find a "good" value of $k$, but it often will not search enough values. To find a good value, you may need to specify the `tuneGrid` option. This option takes a dataframe of parameter values for which you want to check the fit: 

```{r}
library(caret)

knn = train(
    schnitzel ~ beer, 
    data=oktoberfest, 
    method='knn', 
    trControl=trainControl(method='cv'), 
    tuneGrid=data.frame(k=1:18)
)
```

:::{.callout-note}
As a general rule, if the "best" parameter you find sits at the very biggest (or very smallest), you may need to increase (or decrease) the upper (or lower) limit of the values you're searching! For KNN, $k>0$, so you can at least stop the search when $k=1$.
:::

Objects fit using `caret::train()` always show a results table that contains many of the common scores/losses you might want to consider. Showing the first three rows:
```{r}
knn$results |> head(3) |> knitr::kable()
```


This makes it easy to plot the accuracy of the learner as a function of $k$:
```{r}
ggplot(knn$results, aes(x=k, y=RMSE)) + 
    geom_line() + geom_point()
```

From this plot, you might think that choosing any $k<6$ is fairly defensible. By default, caret picks the model with the best RMSE and stores it under the `finalModel` attribute:

```{r}
knn$finalModel
```

The `caret::train()` function picks models with the lowest RMSE by default. This might not always pick the same model as other loss/gain functions. For example, we would pick a different model if we wanted to optimize our R-squared: 

```{r}
ggplot(knn$results, aes(x=k, y=Rsquared)) + 
    geom_line() + geom_point()
```


You can switch to other gain functions using the `maximize` argument:

```{r}
knn_abs = train(
    schnitzel ~ beer, 
    data=oktoberfest, 
    method='knn',
    metric="Rsquared",
    maximize = TRUE,
    trControl=trainControl(method='cv'), 
    tuneGrid=data.frame(k=1:18)
)
knn_abs$finalModel
```

:::{.callout-note}
Note that the exact value of $k$ can change depending on the random changes between cross-validation splits. Try proving this to yourself by training a knn model twice using the same options. In large data, this generally averages out, but to ensure you get the same results every time, make sure to give the `set.seed()` a number, like your student number, your birthday, or some other number, to ensure that the randomness is the same every time. Usually, we set the seed once at the start, or once per chunk if you're repeating that chunk over and over again to troubleshoot. 
:::

You can use this model to make predictions, too: 
```{r}
new_data = tibble(beer=1:4)
preds = predict(knn$finalModel, new_data)

new_data |> mutate(prediction=preds) |> knitr::kable()
```

:::{.callout-tip}
## Comprehension Check
1. Fit the $k=1$, $k=4$, $k=12$, and $k=23$ models from the plot above. 
    a. Which one has the best validation error? You can use `yardstick::rsme_vec(prediction, known)`, which is the `rmse_vec()` function from the `yardstick` package, to calculate the RMSE of a vector of predictions and a vector of known values.  
    b. Does the model that has the lowest validation error match the one you expected to have the best bias-variance tradeoff? If they differ, why do you think they differ?
2. Fit a KNN model to the `Boston` data from ISL, predicting `medv` using `lstat`.
    a. What is the optimum $k$ value? 
    b. What is the predicted `medv` when `lstat` is 5, 10, and 15?
    c. How do these predictions differ from a linear model's predictions? 
    d. Which model do you expect to have a better model fit to *new data*?
:::


## Common pitfalls with KNN

One common issue with KNN learners is that they are *interpolators*. That is, a KNN learner will never predict outside of the range of $y$ that has been observed. Many common models, such as random forests and decision trees which we will consider later in this unit, are also interpolators. These algorithms are often very good at achieving high accuracy when predicting *inside of* the range of the observed $y$ variable, but are generally unreliable for forecasting. 

Another common issue with KNN learners is that they do not handle categorical predictors well. A KNN algorithm can easily make *predictions* about categorical variables, but they have a hard time dealing with categorical $x$ variables. This is because of the issue of *ties*. Imagine you're predicting the price of a car based on its color. You have five "red" cars, seven "green" cars, and ten "blue" cars, all worth different prices. When $k=3$ and you're predicting about a "blue" car, which of the ten training points do you pick? All of them? Then, it's not a "knn" learner! If you want to be strict about $k$, you then must pick $k$ values *at random* when predicting a value for a "blue" car when $k$ is smaller than the number of observations within a group. Second, imagine if $k=8$ and you're predicting the value of a green car. Is "red" closer to "green" than "black"? If you can't decide, then do you pick at random from the "red" cars? or the "black" cars? or, do you pick randomly from all of the red and black cars? At its core, KNN learners work best when *distance is a natural, simple-to-interpret function on the $x$ variables*. For categorical $x$ or $x$ with many ties, this is often not the case, and this will make predictions based on a KNN learner less useful.

Another common issue with KNN is that they are *scale sensitive*. This means that the $k$ value that is discovered depends on the magnitude of $x$. This is *not* a problem when there is only one predictor, but as soon as there is more than one predictor, this can become a problem. 

:::{.callout-tip}
## Comprehension check
1. Fit a KNN model to the `Boston` data from ISL, predicting `medv` using `lstat` and `age`. 
    a. What is the best value of $k$?
    b. Now, force the variables to be re-scaled to have a mean of zero and a variance of one using the `preProcess='scale'` option for `caret::train()`. What is the best value of $k$ there? Is the error higher or lower?
    c. What is the standard deviation of the `lstat` feature? How about the `age` feature? 
    d. If a test point is 1 unit away from a training point in model (a), what's the largest difference in `age` between the test point and the training point? How about `lstat`?
    e. If a test point is 1 unit away from a training point in model (b), what's the largest difference in `age` between the test point and training point? How about `lstat`?
    f. Given what you learned about (d) and (e), is the model from part (a) more sensitive to `age` or `lstat`? How about the model in part (b)?
:::